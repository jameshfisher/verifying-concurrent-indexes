<section xmlns="http://www.w3.org/1999/xhtml"
	 xmlns:xi="http://www.w3.org/2001/XInclude"
	 xmlns:e="http://eegg.github.com/htmlx"
	 xmlns:m="http://eegg.github.com/macro">
  <e:h>Index and set algorithms</e:h>

  <p>
    In the previous section,
    the function calls, and their specifications,
    did not reveal anything about
    what <code>i</code> looks like <q>inside</q>:
    we don't know how memory is laid out,
    or even what <em>parts</em> of memory are allocated for this.
    Nor did we specify what a call to <code>search</code> (for example) actually <em>does</em>,
    and nor did we specify how long it might take
    (nor even if it halts at all!).
  </p>

  <p>
    This was necessary and desirable.
    It was desirable because it enables the programmer to
    reason about the index as a mathematical construct.<e:note>In various terminologies, our <code>Index</code> <e:api /> uses <q>information hiding</q>, also called <q>encapsulation</q>.</e:note>
    It was necessary in order to separate specification from implementation.
    This is particularly important in the case of indexes and sets,
    where a large number of data structures exist that satisfy our <e:api />.
    The following is a very partial list:

    <ul class="listTable listTable4">
      <li>Array</li>
      <li>Association list</li>
      <li>Radix tree</li>
      <li>van Emde Boas tree</li>
      <li>Skip list</li>
      <li>Hash table</li>
      <li>Binary tree</li>
      <li>Splay tree</li>
      <li>AVL tree</li>
      <li>Bloomier filter</li>
      <li>Red-Black Tree</li>
      <li>AA-tree</li>
      <li>LLRB-tree</li>
      <li>B-tree</li>
      <li>B<sup>+</sup>-tree</li>
      <li>B<sup>*</sup>-tree</li>
    </ul>
  </p>

  <p>
    After examining of a few of these,
    it will become quickly evident that
    most <em>index</em> data structures are simply <em>set</em> structures
    with values <q>plugged in</q> next to the key.
    An index <e:const n="I"/> : <e:fpfun><e:fst>K</e:fst><e:snd>V</e:snd></e:fpfun> can be represented by
    a set of tuples <e:math><e:tuple>k, v</e:tuple></e:math>
    (where <e:in><e:fst><e:var n="k"/></e:fst><e:snd><e:const n="K"/></e:snd></e:in> and <e:in><e:fst><e:var n="v"/></e:fst><e:snd><e:const n="V" /></e:snd></e:in>),
    with the restriction that no two elements have the same <e:var n="k"/> value.
    Where the algorithms take this approach,
    for simplicity I instead describe them as implementing a set.
  </p>

  <p>
    Each approach has its own performance and memory characteristics.
    Some have certain restrictions and changes in semantics:
    that the universe of keys must be finite, for instance;
    or that multiple values of the same key can exist.
    A good implementation of a <em>generic</em> index—one
    that implements the API we described,
    for any universe of keys with an ordering—requires <e:bigo><e:var n="n"/></e:bigo> memory
    and guarantees <e:bigo>log(<e:var n="n"/>)</e:bigo> time, or better, for each operation
    (where <e:var n="n"/> is the number of keys in the index).
  </p>

  <p>
    Let's now run through the features of a few index data structures.
    We will then look at specifying a few more formally.
  </p>

  <section>
    <e:h>Array</e:h>

    <p>
      The array is undoubtedly the simplest index algorithm:
      we allocate memory for all members of the universe of keys,
      and initialize all associated values to <code>null</code>.
      It has <e:bigo><e:const n="1"/></e:bigo> complexity for all operations
      as the location of the key in memory is a simple offset operation.
      This is (heavily) paid for in terms of memory,
      the use of which is <e:bigo><e:var n="u"/></e:bigo>, where <e:var n="u"/> is the size of the universe of keys.
      The array is most commonly used where the key type is <code>int</code>,
      but can be used with other types if each key maps to a unique integer.
      Additionally, the array is only possible where the universe of keys is finite—strings,
      for example, are off-limits.
      An array is sensible where
      there is a small allowed key range
      and most or all keys are expected to have a value,
      <m:eg /> a map <e:fpfun><e:fst><code>char</code></e:fst><e:snd><code>char</code></e:snd></e:fpfun> for use in a function <code>string toUpper(string)</code>.
      Otherwise, an algorithm with a memory complexity of <e:bigo><e:var n="n"/></e:bigo> is better.
    </p>
  </section>

  <section>
    <e:h>Hash table</e:h>

    <p>
      The hash table attempts to solve the array's <e:bigo><e:var n="u"/></e:bigo> memory use
      by choosing an array size
      and storing more than one key at each location.
      The key's location in the array (its <q>bucket</q>)
      is decided by a hash function <e:var n="h"/>.
      Keys in the same bucket are typically chained in a linked list.
      The function $h$ varies from extremely simple
      (<m:eg />, h(k) = k % n)
      to complex (<m:eg />, cryptographic functions).
      The hash table has <e:bigo><e:var n="n"/></e:bigo> worst-case behavior
      in the case that all keys map to the same bucket.
      By design, it has poor locality of reference—a good hash function
      spreads similar keys evenly over buckets.
      There is no obvious ordered iteration algorithm.
    </p>
  </section>

  <section>
    <e:h>Association list</e:h>

    <p>
      The association list is a linked list of values of type <code>(K, V)</code>.
      It removes the two disadvantages of the array:
      its size is <e:bigo><e:var n="n"/></e:bigo>,
      and the key universe does not have to be finite.
      It pays for this with <e:bigo><e:var n="n"/></e:bigo> time complexity
      (except for <code>insert</code>, where the new element may be appended to the head of the list).
      The association list might be desirable for its simplicity,
      and where the number of elements is expected to be short;
      it is ofted used, for example, in more complex structures including hash tables.
    </p>

    <p>
      There are many modifications to the basic linked list,
      and these apply to the association list too.
      The <q>linking</q> can be done in both directions
      to allow bidirectional traversal.
    </p>
  </section>


  <section>
    <e:h>Binary search tree</e:h>

    <p>
      The <m:bst /> can be seen as an improvement of the linked list,
      arising from the observation that the nodes of an linked list
      can be generalized to contain any number of links.
      Following a link in a linked list
      only reduces the set being searched by one element;
      following one link out of a possible two, however,
      potentially halves the number of elements in the candidate set.
    </p>

    <p>
      The division made at each node is into
      elements smaller than the element being looked at
      and elements larger than it.
      Therefore, the key type of a <m:bst />
      must have an <em>ordering</em>;
      in practise this does not pose a real problem
      as arbitrary data can always be lexicographically ordered.
    </p>

    <p>
      The division that takes place at every step
      gives the <m:bst /> a best-case complexity
      of <e:bigomega>log(<e:var n="n"/>)</e:bigomega> for all operations.
      <e:note>
	The skip list is another, which does this by <q>skipping over</q> more than one node at a time.
	<q>Balancing</q> is done by randomizing the height of nodes.
	I ignore it because it uses randomization, which is ugly.
      </e:note>
      However, the worst-case scenario,
      in which one of the two sets at every node is the empty set,
      is equivalent to a linked list,
      and thus the <m:bst /> still has <e:bigo><e:var n="n"/></e:bigo> complexity.
    </p>
  </section>


  <section>
    <e:h>Splay tree</e:h>

    <p>
      One way to tackle the worst-case of the <m:bst />
      is simply to reduce its relevance
      by ensuring it is a practical corner-case.
      This is by no means the case with the standard <m:bst />:
      data is often inserted in an <q>almost-sorted</q> order;
      In a worst-case <m:bst /> with the smallest value at the root,
      accessing the largest value still exhibits <e:bigo><e:var n="n"/></e:bigo> time complexity.
    </p>

    <p>
      The Splay tree tries to make the worst-case less relevant
      by ensuring it does not happen repeatedly.
      It does this by, on every operation,
      restructuring the tree to move the latest-accessed element to the root.
      This does not change the complexity of the <m:bst /> operations,
      <e:note>Unlike the <m:bst />, it does perform all operations in <em>amortized</em> <e:bigo>log(<e:var n="n"/>)</e:bigo>; but its worst-cases are still <e:bigo>log(<e:var n="n"/>)</e:bigo>.</e:note>
      and instead makes use of the same principle as caching: locality of reference.
      The worst-case is still exhibited in other situations:
      after <code>search</code>ing for all elements in order,
      the tree will be perfectly unbalanced.
    </p>

    <p>
      The Splay tree has the advantage that it is (relatively) simple:
      the data structure is the same as the <m:bst />,
      and the algorithms are the same except for the addition of the <code>splay</code> method.
      Its disadvantages are significant though:
      it does not provide any worst-case improvement,
      and, unlike other index data structures here,
      its <code>search</code> algorithm mutates the tree.
    </p>
  </section>

  <section>
    <e:h>AVL tree</e:h>

    <p>
      The AVL tree improves on the <m:bst />,
      finally reducing all operations to <e:bigtheta>log(<e:var n="n"/>)</e:bigtheta>.
      It does this by ensuring that the height of the two subtrees of any node
      differ by at most one.<e:cite>cormen, p. 296.</e:cite>
      We first define the <em>height</em> of a node n as

      <table class="displaymath">
	<tbody>
	  <tr><td>height(n) = </td>  <td>0</td>                                         <td>if <e:var n="n"/> is null,</td></tr>
	  <tr><td></td>              <td>max(height(n.left), height(n.right)) + 1</td>  <td>otherwise.</td></tr>
	</tbody>
      </table>
    </p>

    <p>
      We then define <e:jargon>balance factor</e:jargon> of a node <e:var n="n"/> as

      <table class="displaymath">
	<tbody>
	  <tr><td>bf(n) = </td>  <td>0</td>                                 <td>if <e:var n="n"/> is null,</td></tr>
	  <tr><td></td>          <td>height(n.left) - height(n.right)</td>  <td>otherwise.</td></tr>
	</tbody>
      </table>
    </p>

    <p>
      We can then define an AVL tree recursively as

      <div class="displaymath">
	<e:pred name="AVLTree"><code>n</code></e:pred> = -1 ≤ bf(<code>n</code>) ≤ 1 ∧ <e:pred name="AVLTree"><code>n.left</code></e:pred> ∧ <e:pred name="AVLTree"><code>n.right</code></e:pred>.
      </div>

      It is the node's balance factor,
      rather than the height,
      that is stored in the node.<e:note>\cite{haskell_avl}, the most-documented and -commented AVL tree source I know of.</e:note>
      (As, by the definition of the AVL tree,
      the balance factor only ever has one of three possible values,
      this can take up only two bits.)
    </p>
  </section>

  <section>
    <e:h>B-tree</e:h>

    <p>
      The B-tree is primarily motivated by a different problem with the <m:bst />:
      the number of storage reads is <e:bigomega>log<sub>2</sub>(<e:var n="n"/>)</e:bigomega>.
      For media with slow seek times and low granularity
      (such as your hard disk),
      this is too high, and wasteful.
      The B-tree's improvement starts by generalizing the binary tree to a <e:var n="k"/>-ary tree,
      reducing the number of reads to <e:bigomega>log<sub>k</sub>(<e:var n="n"/>)</e:bigomega> (where <e:var n="k"/> &gt; 2).
      The full solution, however, naturally leads to
      another method to bound the height of the tree in the order of log(<e:var n="n"/>).
    </p>

    <p>
      A <e:var n="k"/>-ary search tree holds <e:var n="k"/>−1 elements at each node.
      But it is infeasible to demand that each node holds this maximum amount—for
      one thing, we could only store multiples of <e:var n="k"/>−1 elements!
      This must be relaxed so a node can be less than <q>full</q>.
      However, this must not be relaxed so far that
      a node can hold just one (or zero!) elements,
      because the <e:bigomega>log<sub>2</sub>(<e:var n="n"/>)</e:bigomega> storage reads then reappears.
      The B-tree approach demands that each node<e:note>The root node is an exception, and can be anywhere between empty and full.</e:note>
      is between half-full<e:note>The B<sup>*</sup>-tree demands that nodes are at least two-thirds full.</e:note> and full.
      More precisely, we can define a B-tree in terms of an integer <e:var n="t"/>,
      such that for each node in that tree,
      its number of elements <e:var n="e"/> is such that
      t−1 ≤ e ≤ 2t−1.<e:cite>cormen</e:cite>
    </p>

    <p>
      To illustrate why balance naturally falls out of this,
      consider inserting a value.
      Once we have searched for its position,
      we cannot simply create a new node there,
      as this would not satisfy the restriction on the number of elements.
      Instead the value must be inserted into an existing leaf node.
      If space does not exist at the leaf,
      we create space by splitting the node into two at the same level,
      and inserting the previous median value in the parent using the same technique.
      If the splitting does not reach the root, the depth of the tree is unchanged.
      Otherwise, the root splits, and the depth increases by one for all leaf nodes.
      We can therefore add the additional constraint to the B-tree definition
      that all leaves have the same depth,
      from which we can show that
      the height of a B-tree is <e:bigtheta>log(<e:var n="n"/>)</e:bigtheta>.
    </p>

    <p>
      Rather than bubble-up the node-splitting,
      most B-tree algorithms actually do this on the downwards traversal
      to the node at which the element will be inserted.
      The procedure is simple: every full node visited is split before moving down one level,
      and the median element inserted into the parent.
      (We can guarantee that the parent is non-full
      precisely due to the top-down order of the procedure.)
      The effect is not equivalent,
      as the top-down procedure splits all full nodes,
      where the bubble-up procedure only splits as many as necessary.
      However, balance is still maintained:
      as before, the only time that new levels are created is when the root is split.
    </p>
  </section>
</section>
